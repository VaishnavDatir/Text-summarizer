LIMITATION OF EXISTING SYTEM:
Nevertheless, many NLP models still struggle to understand the semantics of context. By analyzing the results of various models on tasks such as MRC, the researchers find that existing models are largely dependent on keyword or phrase matching, which greatly limits their capability to understand context and handle ambiguity.
It is a well-known fact that existing abstractive text summarization models tend to generate false information. Now, this could happen at either entity level (extra entities are generated) or entity relation level (context in which entities occur is incorrectly generated). This paper quantifies factual consistency at entity-level only and leaves the relation level consistency for future work. They propose a metric to quantify such hallucinations made by the model as well as proposes a couple of measures and training schemes that could help the model perform better and generate entity-level factually correct summaries.
